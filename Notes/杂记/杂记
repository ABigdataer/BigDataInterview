`1、搜索引擎`
  Ⅰ、solr
  Ⅱ、lucene
`2、数据分析平台数据来源`
  Ⅰ、服务器日志（Nginx日志、apche日志、服务器系统日志...）
  Ⅱ、业务日志(Log4j的日志、debug日志信息...)
  ⅲ、用户行为数据
  Ⅳ、第三方数据
  ⅴ、网络爬虫数据
  ......
`3、HBase协处理器的使用步骤`
  Ⅰ、继承BaseRegionServer实现内部处理方法
  Ⅱ、打包成jar文件
  ⅲ、重新创建表，在表创建时会自动挂载该协处理器（表在挂载协处理器的时候，回去HBase的根目录下的lib文件夹下面找到jar包）
`4、Hadoop的Runner继承Configured并实现Tool接口`
      extends configure implements Tool
      需要实现其内部方法setConf(Configration conf){...}    任务提交到yarn,在任务执行前将会调用setConf方法，默认读取hadoop集群配置信息
                      getConf(){...}  当启动start-dfs.sh，会生成一个conf对象，且会将hadoop的各种配置文件加载到conf对象，任务执行时会调用该方法
                      run(args){...}  
`5、HBase整合Hive`
   Ⅰ、可能会存在兼容性问题，在创建关联表时报错，需要重新编译hive的lib下的Hive-hbase-handler-***.jar
   Ⅱ、整合后创建关联表，HBase中，不能已存在表，在网hive中插入数据后，HBase也会同步相应数据
   ⅲ、创建外部关联已存在HBase表，可以用HQL语句直接对HBase中数据进行处理分析
   Ⅳ、对HBase与Hive关联的内部表进行disable和drop操作后，会出现可以在hive的指令窗口中查询到已删除的表，但是查找不到数据，也无法删除内部表，只用重新打开
   一个指令窗口.(实际删除操作应该先删除Hive的表再删除HBase对应的表)
`6、Sqoop整合HBase可能存在兼容问题：`
   例如：Sqoop1.4.6只支持HBase1.0.1之前版本的表自动创建功能(--hbase-create-table)
   实际开发环境中基本只使用Sqoop将关系型数据库内数据导入HBase
7、HBase优化
   Ⅰ、预分区
      每一个Region维护着Start Row 和 End Row ,如果加入的数据符合某个Region维护的rowkey范围，则该数据会交付给这个Region维护。如果将分区提前规划好
 ，则可以提高HBase性能。
      (1)手动设定预分区
      (2)生成十六进制序列预分区
      (3)按照文件中设置的规则进行预分区
      (4)使用Java API进行创建分区
   Ⅱ、RowKey设计
      (1)生成随机数、Hash、散列值
      (2)字符串反转
      (3)字符串拼接
   ⅲ、内存优化
      由于HBase本身特性原因，Hbase运行时需要大量内存开销，但也不建议内存分配过大，因为GC太久会导致ReginServer处于长期不可用状态，一般设置为16-48GB
  就可以了(如果内存没有那么多，可以设置为内存的70%左右)，否则会因为框架占用内存过多导致系统内存不足，框架一样会被系统服务拖死。
   Ⅳ、基础优化
      (1)允许在HDFS中追加内容
      (2)优化DataNode允许打开的最大文件数
      (3)优化数据写入效率--比如将大批量数据写入HBase,可以使用Spark的BulkLoad API批量导入数据
      
